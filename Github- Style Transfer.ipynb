{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c14bf3c6-35de-4e44-becb-570eef9db1cc",
   "metadata": {},
   "source": [
    "1. Import Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "510e9ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import cv2    ##for image processing\n",
    "from PIL import Image\n",
    "\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b368597d-d703-4b5e-8e29-1c83fdb1f7f7",
   "metadata": {},
   "source": [
    "2. Image preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcdb2f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_image = cv2.resize(cv2.imread('C:/Users/pjvya/neckarfront.jpg'), (300, 300))\n",
    "style_image = cv2.resize(cv2.imread('C:/Users/pjvya/style_images/Starry_night.jpg'), (300, 300))\n",
    "\n",
    "content_image = tf.image.convert_image_dtype(content_image, tf.float32)# brings image pixels to 0 and 1\n",
    "style_image = tf.image.convert_image_dtype(style_image, tf.float32)\n",
    "\n",
    "content_image = cv2.cvtColor(np.array(content_image), cv2.COLOR_BGR2RGB)\n",
    "style_image = cv2.cvtColor(np.array(style_image), cv2.COLOR_BGR2RGB)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b7aee5-0fa6-4387-969c-3314fbcd6f13",
   "metadata": {},
   "source": [
    "3. Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "227c9407",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fun(style_outputs, content_outputs, style_target, content_target):\n",
    "    \n",
    "    # Define the weights for content and style losses\n",
    "    style_weight = 0.1\n",
    "    content_weight = 100\n",
    "    \n",
    "    # Calculate the content loss\n",
    "    content_loss = content_weight * tf.reduce_mean(tf.square(content_outputs - content_target))\n",
    "    \n",
    "    # Calculate the style loss using the Gram matrix outputs and target Gram matrices\n",
    "    style_losses = []\n",
    "        # Iterate over pairs of style outputs and style targets\n",
    "    for output_, target_ in zip(style_outputs, style_target):\n",
    "        # Calculate the mean squared difference between output and target\n",
    "        individual_loss = tf.reduce_mean(tf.square(output_ - target_))\n",
    "    \n",
    "        # Append the individual style loss to the list\n",
    "        style_losses.append(individual_loss)\n",
    "\n",
    "    # Sum up all the individual style losses to obtain the final style loss\n",
    "    style_loss = style_weight * tf.add_n(style_losses)\n",
    "    \n",
    "    # Combine content and style losses using the defined weights\n",
    "    total_loss = content_loss + style_loss\n",
    "    #print(total_loss)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a286f3b1-a097-4785-8133-575864f59616",
   "metadata": {},
   "source": [
    "4. Gram Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "404ba15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(input_tensor):\n",
    "    # Reshape the input tensor to have a shape of [-1, channels]  \n",
    "    channels = int(input_tensor.shape[-1])    \n",
    "    a = tf.reshape(input_tensor, [-1, channels]) \n",
    "    #print (a)\n",
    "    n = tf.shape(a)[0]\n",
    "    # Calculate the Gram matrix by multiplying the reshaped tensor with its transpose\n",
    "    gram = tf.matmul(a, a, transpose_a=True)\n",
    "    # Normalize the matrix by dividing by the total number of elements in the reshaped tensor\n",
    "    return gram / tf.cast(n, tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1632046c-0fa4-4e13-a1f1-eae69c114602",
   "metadata": {},
   "source": [
    "5. Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f35cc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vgg(input_shape):\n",
    "    \n",
    "    # Load the VGG19 model with specified input shape\n",
    "    vgg = tf.keras.applications.VGG19(include_top=False, weights=\"imagenet\", input_shape=input_shape)\n",
    "    \n",
    "    # Set the model to be non-trainable\n",
    "    vgg.trainable = False\n",
    "    \n",
    "    # Define the content and style layer names\n",
    "    content_layers = ['block4_conv2']\n",
    "    style_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1']\n",
    "    \n",
    "    # Get the output tensor of the content layer\n",
    "    content_layer = content_layers[0]\n",
    "    content_output = vgg.get_layer(content_layer).output \n",
    "    #print(content_output)\n",
    "    # Get the output tensors of the style layers\n",
    "    # Initialize an empty list to store style outputs\n",
    "    style_outputs = []\n",
    "\n",
    "    # Iterate through the style layers and get their respective outputs\n",
    "    for style_layer in style_layers:\n",
    "        style_layer_output = vgg.get_layer(style_layer).output\n",
    "        style_outputs.append(style_layer_output)\n",
    "\n",
    "    # Assign the list of style outputs to the style_output variable\n",
    "    style_output = style_outputs\n",
    "    #print (style_output)    \n",
    "    # Compute the Gram matrices for each style layer's output\n",
    "    # Initialize an empty list to store Gram matrices of style outputs\n",
    "    gramMatrix_style_outputs = []\n",
    "\n",
    "    # Iterate through the style outputs and calculate their Gram matrices\n",
    "    for output_ in style_output:\n",
    "        gramMatrix_style_output = gram_matrix(output_)\n",
    "        gramMatrix_style_outputs.append(gramMatrix_style_output)\n",
    "\n",
    "    # Assign the list of Gram matrices to the gram_style_output variable\n",
    "    gramMatrix_style_output = gramMatrix_style_outputs\n",
    "    #print(gramMatrix_style_output)\n",
    "    # Create a new model that takes the VGG input and outputs content and style features\n",
    "    # Define the inputs of the model\n",
    "    model_inputs = [vgg.input]\n",
    "\n",
    "    # Define the outputs of the model as a list containing content_output and gramMatrix_style_output\n",
    "    model_outputs = [content_output, gramMatrix_style_output]\n",
    "\n",
    "    # Create the model\n",
    "    model = Model(inputs=model_inputs, outputs=model_outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa32076f-99a4-4953-baac-c4c24fa545ea",
   "metadata": {},
   "source": [
    "6. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45bfe88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_model = load_vgg((300, 300, 3))\n",
    "content_target = vgg_model(np.array([content_image*255]))[0]\n",
    "style_target = vgg_model(np.array([style_image*255]))[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ba4193-ec90-4a98-9796-11fc5d6c8b47",
   "metadata": {},
   "source": [
    "7. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96538161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "#optimizer is to minimize the total loss function by updating the generated image.\n",
    "opt = tf.optimizers.Adam(learning_rate=0.001, beta_1=0.9,beta_2=0.999, epsilon=1e-8)\n",
    "losses = []\n",
    "epochs = []\n",
    "def train_step(image, epoch):\n",
    "    total_time = 0\n",
    "    num_iterations = 1\n",
    "\n",
    "    start = time.time()\n",
    "    with tf.GradientTape() as tape:\n",
    "        output = vgg_model(image*255) # call model again: forward pass, \n",
    "        #image is in the range btw 0 and 1 --> multiply with 255 to give the value back\n",
    "        loss = loss_fun(output[1], output[0], style_target, content_target)\n",
    "    gradient = tape.gradient(loss, image)\n",
    "    opt.apply_gradients([(gradient, image)])# optimize gredient and backpropagate image\n",
    "    # cliping btw 0 and 1 so value of output will not be more than 255\n",
    "    image.assign(tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0))\n",
    "    \n",
    "    stop = time.time()\n",
    "    total_time += (stop - start)\n",
    "    num_iterations += 1\n",
    "    if epoch % 100 ==0:\n",
    "        avg_time = total_time / num_iterations\n",
    "        tf.print(f\"Loss = {loss} time = {avg_time} seconds\")\n",
    "        losses.append(loss)\n",
    "        epochs.append(epoch)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a085ce3-c4fa-4f6b-a283-e4219cd62930",
   "metadata": {},
   "source": [
    "8. Import image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "792bead6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 15113010176.0 time = 2.67716121673584 seconds\r\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "image = tf.image.convert_image_dtype(content_image, tf.float32)\n",
    "image_list = [image]\n",
    "image = tf.Variable(image_list) # make all pixels of image changable, [] to make batch\n",
    "for i in range(EPOCHS):\n",
    "    train_step(image, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6650dfe4-a6f7-49e2-ab5b-a8d41b2b2b12",
   "metadata": {},
   "source": [
    "9. Deprocessesing image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb9248b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = tf.clip_by_value(image * 255, 0.0, 255.0)\n",
    "tensor = np.array(tensor, dtype=np.uint8)\n",
    "if np.ndim(tensor)>3:\n",
    "    assert tensor.shape[0] == 1\n",
    "    tensor = tensor[0]\n",
    "generated_image = cv2.cvtColor(tensor, cv2.COLOR_RGB2BGR)\n",
    "output_path = 'C:/Users/pjvya/generated_images/10iterations.jpg'\n",
    "cv2.imwrite(output_path,generated_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138b7796",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
